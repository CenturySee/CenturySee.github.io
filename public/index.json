[{"content":"前一篇文章，讲述了BloombergGPT的数据、模型与训练细节。具体可见：\n这一篇总结下其余章节。\n文章较长，可先查看要点后再查看感兴趣的章节。\n点击【阅读原文】，查看原始PDF。\n要点 对BloombergGPT的评估包含了两部分：金融领域评估与通用领域评估 评估对比的其他大语言模型有GPT-NeoX、OPT、BLOOM、GPT-3 在金融领域任务上，BloombergGPT综合表现最好；在通用任务上，BloombergGPT的综合得分同样优于相同参数量级的其他模型，并且在某些任务上的得分要高于参数量更大的模型 BloombergGPT模型在金融领域取得好效果的同时，并没有以牺牲模型通用能力为代价 对模型定性评估的结果表明，BloombergGPT可以提高工作效率 出于安全性的考虑，BloogbergGPT模型不会被公开，但是模型训练和评估的相关经验和思考会被分享出来 作者认为，对模型效果提升促进最大的三个因素（按影响从高到低排序）分别为精心清洗的数据集、合理的tokenizer、流行的模型结构 评估 文章中对BloombergGPT的评估分成了两部分：金融领域任务与通用任务。这样做的目的也比较直观，就是验证在特定领域预训练后的模型能够在特定领域表现好，同时在通用领域的表现也不会差太多这一观点。\n整体评测任务分布如下：\n同时，文章对比了BloombergGPT、GPT-NeoX、OPT、BLOOM、GPT-3在不同任务上的表现。注意，这里因为GPT-3模型无法获取，故仅在部分通用任务上进行了评测。\n各个模型使用到的token数量、参数数量、计算量如下：\n作者对每一个模型均独立进行了评测，并且在每一个任务中使用相同的标准prompt、相同的样例、不使用任务描述和任何CoT prompt，以保证评测结果的公平性。\n对于有多个答案的任务，文章中采用了**基于似然的分类方法（likelihood-based classification）进行评估；对于其他任务，文章采用贪心解码（greedy decoding）**的方式进行评估。\nholdout loss 作者首先在FinPile数据集预留的部分样本上对各个模型进行了bits per byte的评估。\nbits per byte指标是评估语言模型的一种常见指标，类似于perplexity，取值越小，模型越好。具体计算方法可见How to compute bits per character (BPC)?\n各个模型在各个类型的数据上的bits per byte值如下：\n可以看出，BloombergGPT在金融语料上的bits per byte均好于其他模型，并且在财报（Filings）这个类别上表现尤其突出。这个结果也符合预期。否则可能就没有后面任务对比的必要了。\n金融领域任务 金融领域任务共有6种类型，3种判别式任务、3中生成式任务。具体任务的格式如下：\n文章又将金融领域任务分成了外部任务和Bloomberg内部任务。在每个任务上，作者除了评估模型在任务上的表现，还评估了同一任务下不同模型生成结果之间两两比较的胜率（WR）。\n外部任务 外部任务主要如下：\nConvFinQA，标普500收益报告问答推理 FiQA SA，金融新闻和微博客标题基于方面的情感三分类（正负中） FPB，金融新闻句子级别情感三分类（正负中） Headline，新闻标题在预定义标签下的二分类 NER，信用风险评估数据的命名实体识别 BloombergGPT在上述5个任务中的4个都取得了最好效果，在另外一个取得了第二名；并且在模型两两结果对比的胜率最高，同时在ConvFinQA这个任务上遥遥领先。\n具体得分如下：\nBloomberg内部任务之情感分析 这个任务中的情感分析均为基于方面的情感三分类（aspect-specific sentiment），数据集的内容通过任务名称就可以略知一二。\nBloombergGPT在上述4个数据集上的表现均大幅领先于其他模型。\n具体结果如下：\n探索性任务：NER 注意，这里的NER只涉及到ORG、PER、LOC这三类实体。\n同时探索性任务NER+NED是指识别出实体后再将实体链接到上市公司的股票简称。比如“AAPL announced that they will stop using Intel chips in future products.” 这句话NER的结果是“AAPL, Intel”，NER+NED的结果是 “AAPL, INTC”。\n这两类任务涉及到的数据集包括了7个数据集，分别为BN（Bloomberg BN wire上内容）、BFW（Bloomberg First Word上的内容）、Filings（财报内容）、Headlines（Bloomberg news内容）、Premium（Bloogberg收录 的第三方新闻内容）、Transcripts（公司新闻发布会的文字记录）、Social Media。\n最终，NER任务下，BloombergGPT仅在Headlines这一个数据集上得分最高；但在NER+NED任务下，BloombergGPT在除了Social Media任务的其他任务上均得分第一。\n具体结果如下：\n通用任务 文章在通用任务上做了相当多的对比，这里仅对任务类型和结果做简要描述，详细内容见文章中的5.4~5.7节。\n作者在BIG-bench Hard（BIG-bench的一个子集，仅包含目前模型表现无法超过人类的任务）、常识测试（不提供任何背景知识，仅可以训练时使用的数据）、阅读理解、语言学（消歧、语法识别、蕴含判别等）等任务上进行了测试。\n在BIG-bench Hard任务上，BloombergGPT得分低于参数量更大的PaLM和BLOOM，但是与参数规模类似的GPT-NeoX或OPT66B相比，BloombergGPT的性能更接近BLOOM，这说明开发金融专用的大语言模型并没有明显牺牲其通用能力。\n在常识测试任务中，BloombergGPT在1个任务上取得了第一名，在其余3个任务上取得了第二名（这里未考虑GPT-3）。\n在阅读理解任务上，GPT-3在所有任务上排名第一，BloombergGPT在5/6个任务上排名第二，且得分远高于BLOOM模型。\n在语言学任务上，GPT-3在综合排名第一，BloombergGPT综合排名第二，且综合得分高于BLOOM模型。\n评测总结 在金融领域任务上，BloombergGPT综合表现最好；\n在通用任务上，BloombergGPT的综合得分优于相同参数量级的其他模型，并且在某些任务上的得分要高于参数量更大的模型。\n这都说明，开发金融专用的大语言模型在金融领域取得好效果的同时，并没有以牺牲模型通用能力为代价。\n这一结论也可以给我们一个启示，在其他特定领域，我们也可以开发专用的大语言模型。\n定性评估 作者在文章的第6章还展示了对BloombergGPT定性评估的例子，以展示模型在专业领域带来的促进作用。\n这些列子包括：\nBQL（Bloomberg查询语言）生成，即使用自然语言完成Bloomberg数据库查询，类似NL2SQL 新闻标题提示，辅助记者生成新闻短标题 金融问答 相关工作 这一章从7个方面讲述了大语言模型训练涉及到的各个方面，这里仅以下图作为总结。\n详细内容可以查看《7. Related Work》的内容。\n道德伦理、限制与研究意义 这一章没有太多值得写的，主要就是强调了目前大语言模型可能会生成有害的、有偏见的内容，并且可能存在prompt注入导致信息泄露的风险，Bloomberg在使用大语言模型前后都会做好风控，保证生成内容的准确性。\n同时，BloogbergGPT模型不会被公开，但是模型训练和评估的相关经验和思考会被分享出来。\n总结与展望 文章提出了BloombergGPT——一个金融领域顶级的LLM，并且在训练特定领域大语言模型做出了如下贡献：\n使用领域数据和通用数据的训练方式可以让模型在这两个方面得到平衡的结果 模型参数量参考了Chinchilla scaling laws 公布了相关训练细节 下一步，作者们会在以下方向继续研究：\n金融领域的fine-tuning 使用更无害和更无偏见的语言 研究tokenization方法对模型结果的影响 最后，作者把模型取得目前效果归结于以下三个因素（按影响从高到低排序）：\n精心清洗的内部数据集 tokenizer的选择 流行的模型结构 ","permalink":"/posts/bloomberggpt2/","summary":"前一篇文章，讲述了BloombergGPT的数据、模型与训练细节。具体可见： 这一篇总结下其余章节。 文章较长，可先查看要点后再查看感兴趣的章节","title":"BloombergGPT——评估、相关工作、意义与总结"},{"content":"BloombergGPT是布隆伯格2023年3月30日公开在arXiv的一篇文章——BloombergGPT: A Large Language Model for Finance中涉及到的语言模型，也是金融领域第一个公开发表文章的大语言模型（以下简称“LLM”）。\n由于原始文章较长，在此我们先就论文中涉及到数据集、模型与训练方面的内容做一下介绍。\n要点 BloombergGPT是Bloomberg训练出来的金融大语言模型（LLM for Finance） 模型参数量为500亿，使用了包含3630亿token的金融领域数据集以及3450亿token的通用数据集 隐藏层维度为7680，多头的头数为40 模型采用Unigram tokenizer，AdamW优化器 模型在64个AWS的p4d.24xlarge实例上训练了53天，其中每个p4d.24xlarge实例包含了8块40GB的A100GPU 引言 BloombergGPT是一个有500亿参数、基于BLOOM模型的LLM，过程中采用了一种兼具通用能力和特定领域的方法。\n为了使用这种方法，作者们基于Bloomberg 40年来积累的数据构造了目前最大的金融领域数据集。\n文章的主要贡献在以下几点：\n混合数据集训练方法不仅可以在特定任务上表现出色，也可以在一般NLP基准测试上表现良好 不同于常见的网络爬取数据，本文的数据包含了巨量的可信来源的精心清洗的数据 不仅包含了模型在基准测试集上的评估结果，也包含了在Bloomberg内部任务上的评估结果 在超过7000亿个token的语料库中的5690亿个token上训练出一个500亿参数的LLM 使用Unigram模型而非常用的基于贪心合并的子词标记器进行tokenize，方便在推理时进行更智能的标记化 借鉴BLOOM的训练大模型方法，同时也将自己自己在训练BloombergGPT中的经验分享 数据集 BloombergGPT是一个有500亿参数、基于BLOOM模型的LLM，过程中采用了一种兼具通用能力和特定领域的方法。\n作者首先构建了FinPile——一个包含了新闻、档案、网络爬取的新闻稿件、英文财经文档等英文金融文档的金融领域数据集，同时也采用了通用的数据集。\n金融领域数据集 金融领域数据集共包含了3630亿个token，占总数据集token量的54.2%，具体由以下几个部分构成：\n金融领域相关网页，2980亿token，占比42.01% 金融领域知名新闻源，380亿token，占比5.31% 公司财报，140亿token，占比2.04% 金融相关公司的出版物，90亿token，占比1.21% bloomberg，50亿token，占比0.7% 因为包含一部分收费和私有数据，所以这份数据集不会被公开，但是文章中公开了模型训练方法。\n通用数据集 通用数据集共包含了3450亿个token，占总数据集token量的48.73%，具体分为如下几个部分：\nThe Pile数据集，1840亿token，占比25.9% C4数据集，1380亿token，占比19.48% Wikipedia数据集，240亿token，占比3.35% 数据集使用Unigram tokenizer对原始文本进行tokenize。具体处理时，作者这了两点改进（具体内容可参考原论文《2.3Tokenization》）：\n在pretokenization这一步，将数字视为单个token，并且允许词组的存在，以提高信息密度减少句子长度\n使用分治的思想优化Unigram tokenizer在大数据集上的实现，并对最终词表大小控制在13万这个数量级上\n模型 模型结构 模型基于BLOOM模型的自回归结构，具体包含了70层transformer decoder。\n另外一些细节如下（详见《3.1 Architecture》）：\n前馈层（FFN）中的非线性函数采用GELU 位置编码采用ALiBi编码 模型在第一层多了一个layer normalization 模型尺度 这一部分，作者先有了算力预算（40G内存A100共130万GPU小时），并且给中间checkpoint存储留出了约25%的时间预算。\n根据Chinchilla scaling laws，计算出模型的参数和需要的数据量大小——模型参数为500亿，token数据量为11000+亿。\n考虑到金融领域token数量要占总token数量的50%以上，而且目前的数据暂时无法再进行扩充，最终模型参数量选择为500亿，token数据量为7000+亿。\n另一方面，隐藏层维度D也可以根据decoder的层数计算出来，这里经过计算隐藏层维度为7680，多头的头数为40。\n训练配置 这一部分原始论文写的比较详细，具体见《3.3 Training Configuration》，这里简单摘要如下：\n作者在每篇文档的最后添加了特殊标记\u0026lt;|endoftext|\u0026gt;，模型训练时选取的句子长度为2048token 训练时采用的优化方法是AdamW，beta1、beta2、weight decay取值分别为0.9、0.95、0.1，初始学习率为6e-5，采用cosine衰减、线性warmup方式 模型参数随机初始化为均值0、标准差0.006588的正态分布，并对MLP的第二层和注意力层输出进行缩放 关于训练的不稳定性，文章中没有描述训练BloombergGPT时采用的方法，只是介绍了相关进展 关于计算使用到的硬件，使用了64个AWS的p4d.24xlarge实例，每个p4d.24xlarge实例包含了8块40GB的A100GPU 大规模优化采用的方法 这一部分中，作者描述了具体优化时采用的方法：ZeRO优化、MiCS、Activation Checkpointing、混合精度训练（Mixed Precision Training）、内核融合（fused kernels）。\n具体见《3.4 Large-scale Optimization》\n经过上述优化，上述硬件的平均算力水平达到了102TFLOPs，训练一步需要32.5秒。\n训练过程 损失函数随训练步数变化曲线如下图所示：\n文章中记录模型共训练了139,200步，进行了约0.8个epoch，训练了53天。\n一个epoch都没有训练完的原因是这时验证集上的损失函数已经不再继续下降了。\n具体训练过程如下：\n初始训练的batch size大小为1024，warm-up过程持续了7200步，随后作者将batch size修改为2048。 115,500步之后，验证集上的损失不再下降，然后作者将学习率缩小为原始的2/3； 129,900步之后，学习率缩小为之前的1/2，同时增加dropout 137,100步之后，学习率再次缩小为之前的1/2 最终，训练在146,000步结束。作者选取139,200这一步的模型最为最终使用的模型 这里推荐阅读原始文章3.3节与3.4节中关于训练方法的描述，对于大模型训练有一定的参考意义。\n关于模型的评估，下一篇文章再详细写一下。\n","permalink":"/posts/bloomberggpt1/","summary":"BloombergGPT是布隆伯格2023年3月30日公开在arXiv的一篇文章——BloombergGPT: A Large Language Model for Finance中涉及","title":"BloombergGPT——数据、模型、训练细节"}]